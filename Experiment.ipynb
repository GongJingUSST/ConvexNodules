{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from support.datatools import *\n",
    "from support.paths import PATH\n",
    "from objectives.logist import objective\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "from coordinator import Coordinator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEST_FOLD = str(2)\n",
    "VALID_FOLD = str(3)\n",
    "SHIFT = 2\n",
    "BATCH_SIZE = 32\n",
    "NB_EPOCH = 20\n",
    "OVERSAMPLING = .7\n",
    "UNDERSAMPLING = 8\n",
    "LOWER = -1000\n",
    "UPPER = 400\n",
    "IN_SHAPE = (1, 22, 22)\n",
    "WEIGHTS = '/home/a.dobrenkii/Projects/Kaggle/DataScienceBowl2K17/data/WEIGHTS/'\n",
    "CPU = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train, valid, test = extract_paths(VALID_FOLD, TEST_FOLD)\n",
    "train_generator = batch_generator(train,\n",
    "                                  batch_size=BATCH_SIZE, \n",
    "                                  in_shape=IN_SHAPE,\n",
    "                                  lower=LOWER,\n",
    "                                  upper=UPPER,\n",
    "                                  shift=SHIFT,\n",
    "                                  undersampling=UNDERSAMPLING,\n",
    "                                  oversampling=OVERSAMPLING,\n",
    "                                  CPU=CPU)\n",
    "\n",
    "new_test = manipulate_samples(test.tolist(), UNDERSAMPLING, 1)\n",
    "test_generator = batch_generator(new_test,\n",
    "                                 batch_size=BATCH_SIZE, \n",
    "                                 in_shape=IN_SHAPE,\n",
    "                                 lower=LOWER,\n",
    "                                 upper=UPPER,\n",
    "                                 shift=0, \n",
    "                                 undersampling=0,\n",
    "                                 oversampling=1,\n",
    "                                 CPU=CPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here $lr == 1/L$  \n",
    "Define prox-function $d(\\vec{x}) \\equiv ||\\, \\vec{x}\\, ||^{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L-BFGS two-loop recursion:\n",
    "Input: $\\vec{∇}f(x_i),\\: s_k,\\: y_k$ where $k = i − m,\\: ...,\\: i − 1$   \n",
    "Output: new direction p  \n",
    "$ p = −\\vec{∇}f(x_i);$  \n",
    "for $k \\leftarrow i − 1$ to $i − m$ do   \n",
    "$\\qquad \\alpha_i \\leftarrow \\frac{s_k \\cdot p}{s_k \\cdot y_k };$   \n",
    "$\\qquad p = p - \\alpha_k \\cdot y_k;$   \n",
    "end\n",
    "\n",
    "for $k \\leftarrow i − m$ to $i − 1$ do   \n",
    "$\\qquad \\beta = \\frac{y_i \\cdot s_i}{y_i \\cdot p_i};$   \n",
    "$\\qquad p = p + (\\alpha_i − \\beta) \\cdot s_i;$   \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class optimizer:\n",
    "    def __init__(self, method, objective, lr=.01, history_len=0):\n",
    "        self.methods = {\n",
    "            \"SGD\": self.SGD,\n",
    "            \"FGD\": self.FGD,\n",
    "            \"L_BFGS\": self.L_BFGS\n",
    "        }\n",
    "\n",
    "        message = \"The method param should match one of: \" + ', '.join(self.methods.keys())\n",
    "        assert method in self.methods.keys(), message\n",
    "        message = \"The param history_len should be positive while using BFGS-like methods\"\n",
    "        assert \"BFGS\" in method and history_len, message\n",
    "        \n",
    "        self.i = 0\n",
    "        self.lr = lr\n",
    "        self.method =  self.methods[method]\n",
    "        self.objective = objective\n",
    "        self.history_len = history_len\n",
    "        self.grads_hist = array(0)\n",
    "        self.grad_i = array(0)\n",
    "        \n",
    "        if history_len != 0:\n",
    "            self.y_diffs = deque([], maxlen=self.history_len)\n",
    "            self.s_diffs = deque([], maxlen=self.history_len)\n",
    "            self.a_hist = deque([], maxlen=self.history_len)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def iteration_prior(self, X, y):\n",
    "        self.i += 1\n",
    "        self.grad_prev = self.grad_i.copy()\n",
    "        self.w = self.objective.w.copy()\n",
    "        self.grad_i = self.objective.gradf(X, y)\n",
    "        \n",
    "        \n",
    "    def __call__(self, X, y):\n",
    "        self.iteration_prior(X, y)\n",
    "        self.method(X, y)\n",
    "\n",
    "        \n",
    "    def SGD(self, X, y):\n",
    "        self.objective.w -= self.lr * self.grad_i\n",
    "    \n",
    "    \n",
    "    def FGD(self, X, y):\n",
    "        # Here lr == 1/L\n",
    "        y_i = self.objective.w - self.lr * self.grad_i\n",
    "        # Define prox-function d(\\vec{x}) \\equiv || \\vec{x} ||^{2}\n",
    "        self.grads_hist += (self.i + 1) / 2 * self.grad_i\n",
    "        z_i = - self.lr * self.grads_hist\n",
    "        self.objective.w = 2. /(self.i + 3.) * z_i + (self.i + 1.) / (self.i + 3.) * y_i\n",
    "    \n",
    "\n",
    "    def L_BFGS(self, X, y):\n",
    "        \"\"\"\n",
    "        Large scale BFGS using two-loop recursion\n",
    "    \n",
    "        \"\"\"\n",
    "        if self.i > self.history_len:\n",
    "            p = - self.grad_i\n",
    "            for y_i, s_i in zip(self.y_diffs, \n",
    "                                self.s_diffs):\n",
    "                self.a_hist.append(dot(s_i, p) / dot(y_i, s_i))\n",
    "                p -= self.a_hist[-1] * y_i\n",
    "                \n",
    "            p *= (dot(self.y_diffs[-1], self.s_diffs[-1]) \n",
    "                  / dot(self.y_diffs[-1], self.y_diffs[-1]))\n",
    "            print(self.y_diffs[-1].shape)\n",
    "            ids = list(reversed(arange(self.history_len)))\n",
    "            for i in ids:\n",
    "                b = dot(self.s_diffs[i], p) / dot(self.y_diffs[i], self.s_diffs[i])\n",
    "                p += (self.a_hist[i] - b) * self.s_diffs[i]\n",
    "                \n",
    "            self.objective.w += self.lr * p\n",
    "        else:\n",
    "            self.SGD(X, y)\n",
    "            \n",
    "        self.y_diffs.append(self.grad_i - self.grad_prev)\n",
    "        self.s_diffs.append(self.objective.w - self.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "objf = objective(dim=484, w=None, l1=1e-4, l2=1e-4)\n",
    "optf = optimizer('L_BFGS', objf, history_len=20)\n",
    "clf = Coordinator(objf, optf, 'logloss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "history = clf.fit_generator(train_generator, \n",
    "                            10 * len(new_test) // BATCH_SIZE, \n",
    "                            nb_epoch=10, \n",
    "                            validation_data=test_generator, \n",
    "                            nb_val_iterations=len(new_test) // BATCH_SIZE, \n",
    "                            verbose=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
